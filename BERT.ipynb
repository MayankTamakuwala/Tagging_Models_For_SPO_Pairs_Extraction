{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "55729.52s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.50.3)\n",
      "Requirement already satisfied: seqeval in ./.venv/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: hf_xet in ./.venv/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.11/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from spacy) (70.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas spacy transformers seqeval torch scikit-learn hf_xet\n",
    "# %python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"globenewswire_articles_finance.csv\")\n",
    "triplets_list = []\n",
    "\n",
    "with open(\"finance_articles_triplets.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        if not row:\n",
    "            continue\n",
    "        \n",
    "        url = row[0]\n",
    "        triplet_fields = []\n",
    "        \n",
    "        for field in row[1:]:\n",
    "            if field.strip():\n",
    "                str_tuple = field.strip()\n",
    "                if str_tuple.startswith('(') and str_tuple.endswith(')'):\n",
    "                    inner_str = str_tuple[1:-1]\n",
    "                    elements = [elem.strip() for elem in inner_str.split(',')]\n",
    "                    triplet_fields.append(tuple(elements))\n",
    "                else:\n",
    "                    triplet_fields.append((str_tuple,))\n",
    "\n",
    "        triplets_list.append({\"url\": url, \"triplets\": triplet_fields})\n",
    "\n",
    "triplets_df = pd.DataFrame(triplets_list)\n",
    "merged_df = pd.merge(triplets_df, articles_df[[\"url\", \"content\"]], on=\"url\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tags(text, spo_list):\n",
    "    tokens = tokenize_text(text)\n",
    "    tags = ['O'] * len(tokens)\n",
    "\n",
    "    for spo in spo_list:\n",
    "        try:\n",
    "            subject, predicate, obj = spo\n",
    "            spans = {\n",
    "                'SUB': subject.split(),\n",
    "                'PRED': predicate.split(),\n",
    "                'OBJ': obj.split()\n",
    "            }\n",
    "\n",
    "            for label, span_tokens in spans.items():\n",
    "                for i in range(len(tokens) - len(span_tokens) + 1):\n",
    "                    if tokens[i:i+len(span_tokens)] == span_tokens:\n",
    "                        tags[i] = f'B-{label}'\n",
    "                        for j in range(1, len(span_tokens)):\n",
    "                            tags[i + j] = f'I-{label}'\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for idx, row in articles_df.iterrows():\n",
    "    url = row['url']\n",
    "    content = row['content']\n",
    "\n",
    "    matching_triplets_row = triplets_df[triplets_df['url'] == url]\n",
    "    if matching_triplets_row.empty:\n",
    "        continue\n",
    "\n",
    "    tokens, tags = get_bio_tags(content, triplets_df.iloc[idx, 1])\n",
    "    dataset.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Unique BIO tags\n",
    "tag_values = ['O', 'B-SUB', 'I-SUB', 'B-PRED', 'I-PRED', 'B-OBJ', 'I-OBJ']\n",
    "tag2id = {tag: i for i, tag in enumerate(tag_values)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPOBioDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, tag2id, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, tags = self.data[idx]\n",
    "\n",
    "        tokenized_input = self.tokenizer(tokens,\n",
    "                                        is_split_into_words=True,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        max_length=self.max_len,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "        word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(tag2id.get(tags[word_idx], tag2id['O']))\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_input['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized_input['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label_ids)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = SPOBioDataset(train_data, tokenizer, tag2id)\n",
    "val_dataset = SPOBioDataset(val_data, tokenizer, tag2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BERTTagger(nn.Module):\n",
    "    def __init__(self, tag2id):\n",
    "        super(BERTTagger, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, len(tag2id))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "def compute_class_weights(dataset, tag2id):\n",
    "    tag_counts = Counter(tag for _, tags in dataset for tag in tags)\n",
    "    total = sum(tag_counts.values())\n",
    "    weights = [1.0 - (tag_counts[tag] / total) for tag in tag2id.keys()]\n",
    "    print(weights)\n",
    "    weights[0] += 0.1\n",
    "    print(weights)\n",
    "    return torch.tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01763469908449644, 0.9976737205463004, 0.9973735554555005, 0.9966231427285007, 0.9992245735154335, 0.9959727850317674, 0.9954975236380009]\n",
      "[0.11763469908449645, 0.9976737205463004, 0.9973735554555005, 0.9966231427285007, 0.9992245735154335, 0.9959727850317674, 0.9954975236380009]\n"
     ]
    }
   ],
   "source": [
    "model = BERTTagger(tag2id).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "weights = compute_class_weights(train_data, tag2id)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.0931\n",
      "Epoch 2 | Loss: 0.7601\n",
      "Epoch 3 | Loss: 0.5948\n",
      "Epoch 4 | Loss: 0.4629\n",
      "Epoch 5 | Loss: 0.3518\n",
      "Epoch 6 | Loss: 0.2550\n",
      "Epoch 7 | Loss: 0.2307\n",
      "Epoch 8 | Loss: 0.1814\n",
      "Epoch 9 | Loss: 0.1420\n",
      "Epoch 10 | Loss: 0.0996\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, len(tag2id)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} | Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCKHOLM, SWEDEN – March 12, 2025. Karolinska Development AB (Nasdaq Stockholm: KDEV) today announces that portfolio company PharmNovo received positive feedback regarding its most advanced drug candidate, PN6047, in a pre-IND meeting with the U.S. Food and Drug Administration (FDA). The meeting aimed to provide guidance on the design of the company's planned Phase 2a clinical trial for the treatment of peripheral neuropathy and allodynia. PharmNovo conducted a regulatory pre-IND Type B meeting with the FDA in January 2025. Such meetings are typically held before submitting an Investigational New Drug (IND) application, which is required to conduct clinical studies in the U.S. During the meeting, PharmNovo presented preclinical data, sought advice on its Chemistry, Manufacturing, and Controls (CMC) activities, and received guidance on the design of its planned Phase 2a study for PN6047. FDA did not direct any negative remarks neither concerning the CMC information nor the preclinical data and provided useful guidance for the continued clinical development Based on the feedback, PharmNovo plans to apply for IND approval from FDA before the end of 2025. Furthermore, the company aims to apply for approval to initiate a clinical Phase 2a trial in Europe in the third quartal of 2025. PharmNovo’s most advanced drug candidate, PN6047, is a selective delta-opioid receptor agonist (DORA) being developed as a new treatment for complex pain conditions. \"Pain management is an area of significant commercial interest. We are therefore very pleased with the outcome of PharmNovo’s pre-IND meeting with the FDA and look forward to following the company's continued efforts to launch its planned Phase 2a clinical study,\" said Viktor Drvota, CEO of Karolinska Development. Karolinska Development's ownership in PharmNovo amounts to 20%.  For further information, please contact: Viktor Drvota, CEO, Karolinska Development AB Phone: +46 73 982 52 02, e-mail: viktor.drvota@karolinskadevelopment.com Johan Dighed, General Counsel and Deputy CEO, Karolinska Development ABPhone: +46 70 207 48 26, e-mail: johan.dighed@karolinskadevelopment.com TO THE EDITORS About Karolinska Development ABKarolinska Development AB (Nasdaq Stockholm: KDEV) is a Nordic life sciences investment company. The company focuses on identifying breakthrough medical innovations in the Nordic region that are developed by entrepreneurs and leadership teams. The company invests in the creation and growth of companies that advance these assets into commercial products that are designed to make a difference to patients' lives while providing an attractive return on investment to shareholders. Karolinska Development has access to world-class medical innovations at the Karolinska Institutet and other leading universities and research institutes in the Nordic region. The Company aims to build companies around scientists who are leaders in their fields, supported by experienced management teams and advisers, and co-funded by specialist international investors, to provide the greatest chance of success. Karolinska Development has a portfolio of eleven companies targeting opportunities in innovative treatment for life-threatening or serious debilitating diseases. The company is led by an entrepreneurial team of investment professionals with a proven track record as company builders and with access to a strong global network. For more information, please visit www.karolinskadevelopment.com. Attachment\n",
      "Predicted Triplets:\n",
      "('Karolinska', 'announces', 'PN6047')\n",
      "('Karolinska', 'announces', 'Europe')\n",
      "('Karolinska', 'received', 'PN6047')\n",
      "('Karolinska', 'received', 'Europe')\n",
      "('Karolinska', 'conducted', 'PN6047')\n",
      "('Karolinska', 'conducted', 'Europe')\n",
      "('Karolinska', 'plans', 'PN6047')\n",
      "('Karolinska', 'plans', 'Europe')\n",
      "('Karolinska', 'aims', 'PN6047')\n",
      "('Karolinska', 'aims', 'Europe')\n",
      "('Karolinska', 'is', 'PN6047')\n",
      "('Karolinska', 'is', 'Europe')\n",
      "('Karolinska', 'launch', 'PN6047')\n",
      "('Karolinska', 'launch', 'Europe')\n",
      "('Karolinska', 'said', 'PN6047')\n",
      "('Karolinska', 'said', 'Europe')\n",
      "('Karolinska', 'contact', 'PN6047')\n",
      "('Karolinska', 'contact', 'Europe')\n",
      "('Karolinska', 'announces', 'PN6047')\n",
      "('Karolinska', 'announces', 'Europe')\n",
      "('Karolinska', 'received', 'PN6047')\n",
      "('Karolinska', 'received', 'Europe')\n",
      "('Karolinska', 'conducted', 'PN6047')\n",
      "('Karolinska', 'conducted', 'Europe')\n",
      "('Karolinska', 'plans', 'PN6047')\n",
      "('Karolinska', 'plans', 'Europe')\n",
      "('Karolinska', 'aims', 'PN6047')\n",
      "('Karolinska', 'aims', 'Europe')\n",
      "('Karolinska', 'is', 'PN6047')\n",
      "('Karolinska', 'is', 'Europe')\n",
      "('Karolinska', 'launch', 'PN6047')\n",
      "('Karolinska', 'launch', 'Europe')\n",
      "('Karolinska', 'said', 'PN6047')\n",
      "('Karolinska', 'said', 'Europe')\n",
      "('Karolinska', 'contact', 'PN6047')\n",
      "('Karolinska', 'contact', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'announces', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'announces', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'received', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'received', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'conducted', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'conducted', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'plans', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'plans', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'aims', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'aims', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'is', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'is', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'launch', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'launch', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'said', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'said', 'Europe')\n",
      "('Karolinska Karolinska Development AB', 'contact', 'PN6047')\n",
      "('Karolinska Karolinska Development AB', 'contact', 'Europe')\n",
      "('PharmNovo', 'announces', 'PN6047')\n",
      "('PharmNovo', 'announces', 'Europe')\n",
      "('PharmNovo', 'received', 'PN6047')\n",
      "('PharmNovo', 'received', 'Europe')\n",
      "('PharmNovo', 'conducted', 'PN6047')\n",
      "('PharmNovo', 'conducted', 'Europe')\n",
      "('PharmNovo', 'plans', 'PN6047')\n",
      "('PharmNovo', 'plans', 'Europe')\n",
      "('PharmNovo', 'aims', 'PN6047')\n",
      "('PharmNovo', 'aims', 'Europe')\n",
      "('PharmNovo', 'is', 'PN6047')\n",
      "('PharmNovo', 'is', 'Europe')\n",
      "('PharmNovo', 'launch', 'PN6047')\n",
      "('PharmNovo', 'launch', 'Europe')\n",
      "('PharmNovo', 'said', 'PN6047')\n",
      "('PharmNovo', 'said', 'Europe')\n",
      "('PharmNovo', 'contact', 'PN6047')\n",
      "('PharmNovo', 'contact', 'Europe')\n",
      "('PharmNovo', 'announces', 'PN6047')\n",
      "('PharmNovo', 'announces', 'Europe')\n",
      "('PharmNovo', 'received', 'PN6047')\n",
      "('PharmNovo', 'received', 'Europe')\n",
      "('PharmNovo', 'conducted', 'PN6047')\n",
      "('PharmNovo', 'conducted', 'Europe')\n",
      "('PharmNovo', 'plans', 'PN6047')\n",
      "('PharmNovo', 'plans', 'Europe')\n",
      "('PharmNovo', 'aims', 'PN6047')\n",
      "('PharmNovo', 'aims', 'Europe')\n",
      "('PharmNovo', 'is', 'PN6047')\n",
      "('PharmNovo', 'is', 'Europe')\n",
      "('PharmNovo', 'launch', 'PN6047')\n",
      "('PharmNovo', 'launch', 'Europe')\n",
      "('PharmNovo', 'said', 'PN6047')\n",
      "('PharmNovo', 'said', 'Europe')\n",
      "('PharmNovo', 'contact', 'PN6047')\n",
      "('PharmNovo', 'contact', 'Europe')\n",
      "('PharmNovo', 'announces', 'PN6047')\n",
      "('PharmNovo', 'announces', 'Europe')\n",
      "('PharmNovo', 'received', 'PN6047')\n",
      "('PharmNovo', 'received', 'Europe')\n",
      "('PharmNovo', 'conducted', 'PN6047')\n",
      "('PharmNovo', 'conducted', 'Europe')\n",
      "('PharmNovo', 'plans', 'PN6047')\n",
      "('PharmNovo', 'plans', 'Europe')\n",
      "('PharmNovo', 'aims', 'PN6047')\n",
      "('PharmNovo', 'aims', 'Europe')\n",
      "('PharmNovo', 'is', 'PN6047')\n",
      "('PharmNovo', 'is', 'Europe')\n",
      "('PharmNovo', 'launch', 'PN6047')\n",
      "('PharmNovo', 'launch', 'Europe')\n",
      "('PharmNovo', 'said', 'PN6047')\n",
      "('PharmNovo', 'said', 'Europe')\n",
      "('PharmNovo', 'contact', 'PN6047')\n",
      "('PharmNovo', 'contact', 'Europe')\n",
      "('PharmNovo', 'announces', 'PN6047')\n",
      "('PharmNovo', 'announces', 'Europe')\n",
      "('PharmNovo', 'received', 'PN6047')\n",
      "('PharmNovo', 'received', 'Europe')\n",
      "('PharmNovo', 'conducted', 'PN6047')\n",
      "('PharmNovo', 'conducted', 'Europe')\n",
      "('PharmNovo', 'plans', 'PN6047')\n",
      "('PharmNovo', 'plans', 'Europe')\n",
      "('PharmNovo', 'aims', 'PN6047')\n",
      "('PharmNovo', 'aims', 'Europe')\n",
      "('PharmNovo', 'is', 'PN6047')\n",
      "('PharmNovo', 'is', 'Europe')\n",
      "('PharmNovo', 'launch', 'PN6047')\n",
      "('PharmNovo', 'launch', 'Europe')\n",
      "('PharmNovo', 'said', 'PN6047')\n",
      "('PharmNovo', 'said', 'Europe')\n",
      "('PharmNovo', 'contact', 'PN6047')\n",
      "('PharmNovo', 'contact', 'Europe')\n",
      "('Viktor', 'announces', 'PN6047')\n",
      "('Viktor', 'announces', 'Europe')\n",
      "('Viktor', 'received', 'PN6047')\n",
      "('Viktor', 'received', 'Europe')\n",
      "('Viktor', 'conducted', 'PN6047')\n",
      "('Viktor', 'conducted', 'Europe')\n",
      "('Viktor', 'plans', 'PN6047')\n",
      "('Viktor', 'plans', 'Europe')\n",
      "('Viktor', 'aims', 'PN6047')\n",
      "('Viktor', 'aims', 'Europe')\n",
      "('Viktor', 'is', 'PN6047')\n",
      "('Viktor', 'is', 'Europe')\n",
      "('Viktor', 'launch', 'PN6047')\n",
      "('Viktor', 'launch', 'Europe')\n",
      "('Viktor', 'said', 'PN6047')\n",
      "('Viktor', 'said', 'Europe')\n",
      "('Viktor', 'contact', 'PN6047')\n",
      "('Viktor', 'contact', 'Europe')\n",
      "('Viktor', 'announces', 'PN6047')\n",
      "('Viktor', 'announces', 'Europe')\n",
      "('Viktor', 'received', 'PN6047')\n",
      "('Viktor', 'received', 'Europe')\n",
      "('Viktor', 'conducted', 'PN6047')\n",
      "('Viktor', 'conducted', 'Europe')\n",
      "('Viktor', 'plans', 'PN6047')\n",
      "('Viktor', 'plans', 'Europe')\n",
      "('Viktor', 'aims', 'PN6047')\n",
      "('Viktor', 'aims', 'Europe')\n",
      "('Viktor', 'is', 'PN6047')\n",
      "('Viktor', 'is', 'Europe')\n",
      "('Viktor', 'launch', 'PN6047')\n",
      "('Viktor', 'launch', 'Europe')\n",
      "('Viktor', 'said', 'PN6047')\n",
      "('Viktor', 'said', 'Europe')\n",
      "('Viktor', 'contact', 'PN6047')\n",
      "('Viktor', 'contact', 'Europe')\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_bio_tags(text, model, tokenizer, tag2id, id2tag, device):\n",
    "    model.eval()\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    tokenized_input = tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=512)\n",
    "\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    predicted_tags = []\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        tag_id = predictions[0][idx].item()\n",
    "        tag = id2tag[tag_id]\n",
    "        predicted_tags.append((tokens[word_idx], tag))\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "def extract_spans(tagged_tokens):\n",
    "    spans = {'SUB': [], 'PRED': [], 'OBJ': []}\n",
    "    current_span = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag == 'O':\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "                current_span = []\n",
    "                current_label = None\n",
    "        elif tag.startswith('B-'):\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_label = tag[2:]\n",
    "            current_span = [token]\n",
    "        elif tag.startswith('I-') and current_label == tag[2:]:\n",
    "            current_span.append(token)\n",
    "        else:\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_span = []\n",
    "            current_label = None\n",
    "\n",
    "    # Final check\n",
    "    if current_span and current_label:\n",
    "        spans[current_label].append(\" \".join(current_span))\n",
    "\n",
    "    return spans\n",
    "\n",
    "def form_triplets_from_spans(spans):\n",
    "    subs = spans['SUB']\n",
    "    preds = spans['PRED']\n",
    "    objs = spans['OBJ']\n",
    "\n",
    "    triplets = []\n",
    "    for s in subs:\n",
    "        for p in preds:\n",
    "            for o in objs:\n",
    "                triplets.append((s, p, o))\n",
    "\n",
    "    return triplets\n",
    "sample_text = articles_df.iloc[0][\"content\"]\n",
    "\n",
    "print(sample_text)\n",
    "\n",
    "tagged = predict_bio_tags(sample_text, model, tokenizer, tag2id, id2tag, device)\n",
    "spans = extract_spans(tagged)\n",
    "triplets = form_triplets_from_spans(spans)\n",
    "\n",
    "print(\"Predicted Triplets:\")\n",
    "for t in triplets:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
