{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas spacy transformers seqeval torch scikit-learn hf_xet\n",
    "# %python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame()\n",
    "sectors = [\"finance\", \"healthcare\", \"tech\"]\n",
    "triplets_list = []\n",
    "for sector in sectors:\n",
    "    articles = pd.read_csv(f\"globenewswire_articles_{sector}.csv\")\n",
    "    articles_df = pd.concat([articles_df, articles], ignore_index=True)\n",
    "\n",
    "    with open(f\"{sector}_articles_triplets.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            \n",
    "            url = row[0]\n",
    "            triplet_fields = []\n",
    "            \n",
    "            for field in row[1:]:\n",
    "                if field.strip():\n",
    "                    str_tuple = field.strip()\n",
    "                    if str_tuple.startswith('(') and str_tuple.endswith(')'):\n",
    "                        inner_str = str_tuple[1:-1]\n",
    "                        elements = [elem.strip() for elem in inner_str.split(',')]\n",
    "                        triplet_fields.append(tuple(elements))\n",
    "                    else:\n",
    "                        triplet_fields.append((str_tuple,))\n",
    "\n",
    "            triplets_list.append({\"url\": url, \"triplets\": triplet_fields})\n",
    "\n",
    "triplets_df = pd.DataFrame(triplets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tags(text, spo_list):\n",
    "    tokens = tokenize_text(text)\n",
    "    tags = ['O'] * len(tokens)\n",
    "\n",
    "    for spo in spo_list:\n",
    "        try:\n",
    "            subject, predicate, obj = spo\n",
    "            spans = {\n",
    "                'SUB': subject.split(),\n",
    "                'PRED': predicate.split(),\n",
    "                'OBJ': obj.split()\n",
    "            }\n",
    "\n",
    "            for label, span_tokens in spans.items():\n",
    "                for i in range(len(tokens) - len(span_tokens) + 1):\n",
    "                    if tokens[i:i+len(span_tokens)] == span_tokens:\n",
    "                        tags[i] = f'B-{label}'\n",
    "                        for j in range(1, len(span_tokens)):\n",
    "                            tags[i + j] = f'I-{label}'\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for idx, row in articles_df.iterrows():\n",
    "    url = row['url']\n",
    "    content = row['content']\n",
    "\n",
    "    matching_triplets_row = triplets_df[triplets_df['url'] == url]\n",
    "    if matching_triplets_row.empty:\n",
    "        continue\n",
    "\n",
    "    tokens, tags = get_bio_tags(content, triplets_df.iloc[idx, 1])\n",
    "    dataset.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Unique BIO tags\n",
    "tag_values = ['O', 'B-SUB', 'I-SUB', 'B-PRED', 'I-PRED', 'B-OBJ', 'I-OBJ']\n",
    "tag2id = {tag: i for i, tag in enumerate(tag_values)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPOBioDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, tag2id, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, tags = self.data[idx]\n",
    "\n",
    "        tokenized_input = self.tokenizer(tokens,\n",
    "                                        is_split_into_words=True,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        max_length=self.max_len,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "        word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(tag2id.get(tags[word_idx], tag2id['O']))\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_input['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized_input['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label_ids)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = SPOBioDataset(train_data, tokenizer, tag2id)\n",
    "val_dataset = SPOBioDataset(val_data, tokenizer, tag2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BERTTagger(nn.Module):\n",
    "    def __init__(self, tag2id):\n",
    "        super(BERTTagger, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, len(tag2id))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "def compute_class_weights(dataset, tag2id):\n",
    "    tag_counts = Counter(tag for _, tags in dataset for tag in tags)\n",
    "    total = sum(tag_counts.values())\n",
    "    weights = [1.0 - (tag_counts[tag] / total) for tag in tag2id.keys()]\n",
    "    print(weights)\n",
    "    # weights[0] += 0.14\n",
    "    weights[0] += 0.1\n",
    "    print(weights)\n",
    "    return torch.tensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.020662489170075182, 0.9973756321653805, 0.9974108923858073, 0.9967812455924725, 0.9991688662327981, 0.9950937921863352, 0.9935070822671315]\n",
      "[0.12066248917007519, 0.9973756321653805, 0.9974108923858073, 0.9967812455924725, 0.9991688662327981, 0.9950937921863352, 0.9935070822671315]\n"
     ]
    }
   ],
   "source": [
    "model = BERTTagger(tag2id).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "weights = compute_class_weights(train_data, tag2id)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.9002\n",
      "Epoch 2 | Loss: 0.5746\n",
      "Epoch 3 | Loss: 0.4213\n",
      "Epoch 4 | Loss: 0.3052\n",
      "Epoch 5 | Loss: 0.2129\n",
      "Epoch 6 | Loss: 0.1574\n",
      "Epoch 7 | Loss: 0.1156\n",
      "Epoch 8 | Loss: 0.0843\n",
      "Epoch 9 | Loss: 0.0741\n",
      "Epoch 10 | Loss: 0.0615\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, len(tag2id)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} | Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_bio_tags(text, model, tokenizer, id2tag, device):\n",
    "    model.eval()\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    tokenized_input = tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=512)\n",
    "\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    predicted_tags = []\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        tag_id = predictions[0][idx].item()\n",
    "        tag = id2tag[tag_id]\n",
    "        if idx == 0 or word_ids[idx] != word_ids[idx - 1]:\n",
    "            predicted_tags.append((tokens[word_idx], tag))\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "def extract_and_form_triplets(text, tagged_tokens):\n",
    "\n",
    "    # Step 1: Extract spans from BIO-tagged tokens\n",
    "    spans = {'SUB': [], 'PRED': [], 'OBJ': []}\n",
    "    current_span = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag == 'O':\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_span = []\n",
    "            current_label = None\n",
    "        elif tag.startswith('B-'):\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_label = tag[2:]\n",
    "            current_span = [token]\n",
    "        elif tag.startswith('I-') and current_label == tag[2:]:\n",
    "            current_span.append(token)\n",
    "        else:\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_span = []\n",
    "            current_label = None\n",
    "\n",
    "    if current_span and current_label:\n",
    "        spans[current_label].append(\" \".join(current_span))\n",
    "\n",
    "    # Step 2: Filter out short or lowercase-only spans\n",
    "    # def filter_spans(spans):\n",
    "    #     def is_valid(span):\n",
    "    #         return len(span.split()) > 1 or span[0].isupper()\n",
    "\n",
    "    #     return {\n",
    "    #         k: [s for s in v if is_valid(s)] for k, v in spans.items()\n",
    "    #     }\n",
    "\n",
    "    # filtered_spans = filter_spans(spans)\n",
    "\n",
    "    filtered_spans = spans\n",
    "\n",
    "    # Step 3: Match spans within same sentence\n",
    "    doc = nlp(text)\n",
    "    triplets = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text\n",
    "        subjs = [s for s in filtered_spans[\"SUB\"] if s in sent_text]\n",
    "        preds = [p for p in filtered_spans[\"PRED\"] if p in sent_text]\n",
    "        objs = [o for o in filtered_spans[\"OBJ\"] if o in sent_text]\n",
    "\n",
    "        for s in subjs:\n",
    "            for p in preds:\n",
    "                for o in objs:\n",
    "                    triplets.append((s, p, o))\n",
    "\n",
    "    return list(set(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Triplets:\n",
      "('PharmNovo', 'is', 'Karolinska Development AB')\n",
      "('Viktor Drvota', 'said', 'FDA')\n",
      "('PharmNovo', 'received', 'Phase')\n",
      "('PharmNovo', 'announces', 'FDA')\n",
      "('PharmNovo', 'announces', 'feedback')\n",
      "('PharmNovo', 'is', 'Phase')\n",
      "('PharmNovo', 'said', 'FDA')\n",
      "('PharmNovo', 'announces', 'Karolinska Development AB')\n",
      "('Viktor Drvota', 'said', 'Phase')\n",
      "('PharmNovo', 'conducted', 'FDA')\n",
      "('PharmNovo', 'plans to apply', 'FDA')\n",
      "('Viktor Drvota', 'said', 'CEO')\n",
      "('PharmNovo', 'plans to apply', 'feedback')\n",
      "('PharmNovo', 'received', 'FDA')\n",
      "('PharmNovo', 'said', 'Phase')\n",
      "('PharmNovo', 'received', 'feedback')\n",
      "('PharmNovo', 'said', 'CEO')\n",
      "('PharmNovo', 'received', 'Karolinska Development AB')\n",
      "('PharmNovo', 'is', 'FDA')\n",
      "('PharmNovo', 'is', 'feedback')\n"
     ]
    }
   ],
   "source": [
    "sample_text = articles_df.iloc[0][\"content\"]\n",
    "\n",
    "# doc = nlp(sample_text)\n",
    "# with open(\"temp1.txt\", \"w\") as f:\n",
    "#     for sent in doc.sents:\n",
    "#         sent_text = sent.text\n",
    "#         print(sent_text)\n",
    "#         f.write(sent_text)\n",
    "#         f.write(\"\\n\")\n",
    "# sample_text = \"SALT LAKE CITY, March  12, 2025  (GLOBE NEWSWIRE) -- Clene, Inc. (Nasdaq: CLNN) and its subsidiary, Clene Nanomedicine, Inc., a clinical-stage biopharmaceutical company focused on revolutionizing the treatment of neurodegenerative diseases, including amyotrophic lateral sclerosis (ALS) and multiple sclerosis (MS), today announced new evidence from a cross-regimen, post hoc analysis of long-term survival in HEALEY ALS Platform Trial participants. The analyses further substantiate that treatment with CNM-Au8¬Æ 30 mg delivers a significant survival benefit for people living with ALS. New Survival AnalysesThe analyses compared survival in participants who received CNM-Au8 30 mg (Regimen C) to those of Regimen A in the HEALEY ALS Platform Trial. Regimen A provided a large concurrent control group vs. CNM-Au8 treatment using the same randomization criteria established within the HEALEY master protocol. Long-term survival status, determined through public records and site reporting, was evaluated over a follow-up period of up to 48 months. 78% of participants across both groups received standard ALS background therapy (riluzole, edaravone, or both) at baseline. These results are consistent with previous survival benefits observed in the HEALEY ALS Platform Trial‚Äôs 24-week double-blind period, the open-label extension of the Phase 2 RESCUE-ALS trial, and analyses of Expanded Access Programs compared to ALS natural history controls. ‚ÄúWe are highly encouraged by these results, as the significant survival advantage demonstrated by CNM-Au8 not only reinforces its potential to extend life for people living with ALS but also validates our strategic direction as we prepare for the launch of our confirmatory Phase 3 RESTORE-ALS study in mid-2025,‚Äù stated Rob Etherington, President and CEO of Clene. ‚ÄúWe look forward to discussing these findings with the FDA as we advance toward commercialization.‚Äù Merit Cudkowicz, M.D., M.S.c., Principal Investigator and sponsor of the HEALEY ALS Platform Trial, director of the Sean M. Healey & AMG Center for ALS, and executive director of the Mass General Brigham Neuroscience Institute, said, ‚ÄúThe innovative design of the HEALEY ALS Platform Trial has enabled us to extract clear and meaningful survival data that helps make decisions about CNM-Au8 drug development.‚Äù About Regimen ARegimen A was one of the first three regimens investigated in the HEALEY ALS Platform Trial. Eligible participants were randomized in a 3:1 ratio to receive active treatment or matching placebo for a planned duration of 24 weeks. Participants assigned to Regimen A had to receive both quadrivalent and serotype B meningococcal vaccinations at least 14 days prior to the first dose of study drug, and participants were excluded from Regimen A if they had a history of meningococcal disease or prior treatment with a complement inhibitor. Regimen A was stopped prematurely for futility after all participants had been randomized, and approximately 70% had completed the Week 24 visit. Participants were instructed to discontinue study dosing, and a final early termination study visit was conducted. Long-term survival status of Regimen A participants was tracked from public records and site reporting independently of the early termination. There was no difference in long-term survival in participants randomized to Regimen A active compared to Regimen A placebo, supporting the combined analyses of the entire Regimen A population for comparisons of long-term survival to CNM-Au8 30 mg (Regimen C) participants. About CleneClene Inc., (Nasdaq: CLNN) (along with its subsidiaries, ‚ÄúClene‚Äù and its wholly owned subsidiary Clene Nanomedicine, Inc.), is a late clinical-stage biopharmaceutical company focused on improving mitochondrial health and protecting neuronal function to treat neurodegenerative diseases, including amyotrophic lateral sclerosis, Parkinson‚Äôs disease, and multiple sclerosis. CNM-Au8¬Æ is an investigational first-in-class therapy that improves central nervous system cells‚Äô survival and function via a mechanism that targets mitochondrial function and the NAD pathway while reducing oxidative stress. CNM-Au8¬Æ is a federally registered trademark of Clene Nanomedicine, Inc. The company is based in Salt Lake City, Utah, with R&D and manufacturing operations in Maryland. For more information, please visit www.clene.com or follow us on X (formerly Twitter) and LinkedIn. About CNM-Au8¬ÆCNM-Au8 is an oral suspension of gold nanocrystals developed to restore neuronal health and function by increasing energy production and utilization. The catalytically active nanocrystals of CNM-Au8 drive critical cellular energy producing reactions that enable neuroprotection and remyelination by increasing neuronal and glial resilience to disease-relevant stressors. CNM-Au8¬Æ is a federally registered trademark of Clene Nanomedicine, Inc. About RESTORE-ALSRESTORE-ALS is a Phase 3 confirmatory global, multi-center, randomized, double-blind, parallel group, placebo-controlled study to evaluate the efficacy, safety, pharmacodynamics, and pharmacokinetics of CNM-Au8 in participants diagnosed with ALS on stable background therapy. The study is designed to investigate the effects of CNM-Au8 on improved survival (primary endpoint) and delayed time to ALS clinical worsening events (secondary efficacy endpoint).¬†Participants will be randomized in a 2:1 ratio to receive either active treatment with CNM-Au8 30 mg or matched placebo daily during the 108-week double-blind treatment period. The Phase 3 RESTORE-ALS clinical trial, due to launch in mid-2025, is planned to serve as the confirmatory clinical trial required to meet the FDA‚Äôs guidance for an ‚Äúunderway‚Äù clinical trial when a New Drug Application requesting Accelerated Approval is submitted. Forward-Looking StatementsThis press release contains ‚Äúforward-looking statements‚Äù within the meaning of Section 21E of the Securities Exchange Act of 1934, as amended, and Section 27A of the Securities Act of 1933, as amended, which are intended to be covered by the ‚Äúsafe harbor‚Äù provisions created by those laws. Clene‚Äôs forward-looking statements include, but are not limited to, statements regarding our or our management team‚Äôs expectations, hopes, beliefs, intentions or strategies regarding our future operations. In addition, any statements that refer to projections, forecasts or other characterizations of future events or circumstances, including any underlying assumptions, are forward-looking statements. The words ‚Äúanticipate,‚Äù ‚Äúbelieve,‚Äù ‚Äúcontemplate,‚Äù ‚Äúcontinue,‚Äù ‚Äúestimate,‚Äù ‚Äúexpect,‚Äù ‚Äúintends,‚Äù ‚Äúmay,‚Äù ‚Äúmight,‚Äù ‚Äúplan,‚Äù ‚Äúpossible,‚Äù ‚Äúpotential,‚Äù ‚Äúpredict,‚Äù ‚Äúproject,‚Äù ‚Äúshould,‚Äù ‚Äúwill,‚Äù ‚Äúwould,‚Äù and similar expressions may identify forward-looking statements, but the absence of these words does not mean that a statement is not forward-looking. These forward-looking statements represent our views as of the date of this press release and involve a number of judgments, risks and uncertainties. We anticipate that subsequent events and developments will cause our views to change. We undertake no obligation to update forward-looking statements to reflect events or circumstances after the date they were made, whether as a result of new information, future events or otherwise, except as may be required under applicable securities laws. Accordingly, forward-looking statements should not be relied upon as representing our views as of any subsequent date. As a result of a number of known and unknown risks and uncertainties, our actual results or performance may be materially different from those expressed or implied by these forward-looking statements. Some factors that could cause actual results to differ include our ability to demonstrate the efficacy and safety of our drug candidates; the clinical results for our drug candidates, which may not support further development or marketing approval; actions of regulatory agencies, which may affect the initiation, timing and progress of clinical trials and marketing approval; our ability to achieve commercial success for our drug candidates, if approved; our limited operating history and our ability to obtain additional funding for operations and to complete the development and commercialization of our drug candidates; and other risks and uncertainties set forth in ‚ÄúRisk Factors‚Äù in our most recent Annual Report on Form 10-K and any subsequent Quarterly Reports on Form 10-Q. In addition, statements that ‚Äúwe believe‚Äù and similar statements reflect our beliefs and opinions on the relevant subject. These statements are based upon information available to us as of the date of this press release, and while we believe such information forms a reasonable basis for such statements, such information may be limited or incomplete, and our statements should not be read to indicate that we have conducted an exhaustive inquiry into, or review of, all potentially available relevant information. These statements are inherently uncertain and you are cautioned not to rely unduly upon these statements. All information in this press release is as of the date of this press release. The information contained in any website referenced herein is not, and shall not be deemed to be, part of or incorporated into this press release.\"\n",
    "\n",
    "tagged = predict_bio_tags(sample_text, model, tokenizer, id2tag, device)\n",
    "# for t in tagged:\n",
    "#     if(t[1] != \"O\"):\n",
    "#         print(t, end=\", \")\n",
    "# print()\n",
    "triplets = extract_and_form_triplets(sample_text, tagged)\n",
    "\n",
    "# with open(\"temp.txt\", \"w\") as f:\n",
    "#     for t in triplets:\n",
    "#         f.write(str(t))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "print(\"Predicted Triplets:\")\n",
    "for t in triplets:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
