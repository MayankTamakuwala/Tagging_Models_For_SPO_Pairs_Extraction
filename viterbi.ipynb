{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8fff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: rouge in ./.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (2.2.5)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.11/site-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ! python -m venv venv\n",
    "# ! source venv/bin/activate\n",
    "# %pip install pandas scikit-learn rouge nltk numpy matplotlib seaborn\n",
    "# OR\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0e44d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mayanktamakuwala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from nltk.tag import hmm\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1990887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path: str) -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Load and preprocess the data from CSV file.\n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "    Returns:\n",
    "        List of sequences, where each sequence is a list of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        \n",
    "        for row in reader:\n",
    "            if not row:  # Skip empty rows\n",
    "                continue\n",
    "                \n",
    "            # Get non-empty columns\n",
    "            selected_columns = [col for col in row[1:] if col.strip()]\n",
    "            \n",
    "            # Process each column to create word-tag pairs\n",
    "            curr = []\n",
    "            for s in selected_columns:\n",
    "                try:\n",
    "                    parts = s.split(\",\")\n",
    "                    if len(parts) != 2:\n",
    "                        continue\n",
    "                        \n",
    "                    word = parts[0].strip().lower()\n",
    "                    tag = parts[1].strip()\n",
    "                    \n",
    "                    if word and tag:  # Only add if both word and tag are non-empty\n",
    "                        curr.append((word, tag))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing column '{s}': {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if curr:  # Only add sequences that have at least one valid word-tag pair\n",
    "                data.append(curr)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a72445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(reference: List[List[str]], candidate: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between reference and candidate sequences.\n",
    "    Args:\n",
    "        reference: List of reference sequences\n",
    "        candidate: List of candidate sequences\n",
    "    Returns:\n",
    "        BLEU score\n",
    "    \"\"\"\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    \n",
    "    for ref, cand in zip(reference, candidate):\n",
    "        # Convert sequences to strings and tokenize\n",
    "        ref_str = ' '.join(ref)\n",
    "        cand_str = ' '.join(cand)\n",
    "        \n",
    "        ref_tokens = nltk.word_tokenize(ref_str)\n",
    "        cand_tokens = nltk.word_tokenize(cand_str)\n",
    "        \n",
    "        # Calculate BLEU score for this pair\n",
    "        score = sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Return average BLEU score\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485f66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(reference: List[List[str]], candidate: List[List[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores between reference and candidate sequences.\n",
    "    Args:\n",
    "        reference: List of reference sequences\n",
    "        candidate: List of candidate sequences\n",
    "    Returns:\n",
    "        Dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    # Convert sequences to sentences\n",
    "    reference_sentences = [' '.join(seq) for seq in reference]\n",
    "    candidate_sentences = [' '.join(seq) for seq in candidate]\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate_sentences, reference_sentences, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6481b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sequence_metrics(reference: List[List[str]], candidate: List[List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate sequence-level metrics.\n",
    "    Args:\n",
    "        reference: List of reference sequences\n",
    "        candidate: List of candidate sequences\n",
    "    Returns:\n",
    "        Dictionary containing sequence-level metrics\n",
    "    \"\"\"\n",
    "    exact_matches = 0\n",
    "    partial_matches = 0\n",
    "    total_sequences = len(reference)\n",
    "    \n",
    "    for ref, cand in zip(reference, candidate):\n",
    "        # Exact match\n",
    "        if ref == cand:\n",
    "            exact_matches += 1\n",
    "        \n",
    "        # Partial match (at least 50% of tags correct)\n",
    "        correct_tags = sum(1 for r, c in zip(ref, cand) if r == c)\n",
    "        if correct_tags / len(ref) >= 0.5:\n",
    "            partial_matches += 1\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_matches / total_sequences if total_sequences > 0 else 0,\n",
    "        'partial_match': partial_matches / total_sequences if total_sequences > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042dea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(true_tags: List[str], pred_tags: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze error patterns in predictions.\n",
    "    Args:\n",
    "        true_tags: List of true tags\n",
    "        pred_tags: List of predicted tags\n",
    "    Returns:\n",
    "        Dictionary containing error analysis\n",
    "    \"\"\"\n",
    "    error_patterns = defaultdict(int)\n",
    "    confusion_pairs = defaultdict(int)\n",
    "    \n",
    "    for true, pred in zip(true_tags, pred_tags):\n",
    "        if true != pred:\n",
    "            error_patterns[(true, pred)] += 1\n",
    "            confusion_pairs[f\"{true}->{pred}\"] += 1\n",
    "    \n",
    "    return {\n",
    "        'error_patterns': dict(error_patterns),\n",
    "        'confusion_pairs': dict(confusion_pairs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943cf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_analysis(error_patterns: Dict, confusion_pairs: Dict):\n",
    "    \"\"\"Plot error analysis visualizations.\"\"\"\n",
    "    # Plot most common error patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    patterns = sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    plt.bar([str(p) for p, _ in patterns], [c for _, c in patterns])\n",
    "    plt.title('Top 10 Error Patterns')\n",
    "    plt.xlabel('(True Tag, Predicted Tag)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_patterns.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion pairs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    plt.bar([p for p, _ in pairs], [c for _, c in pairs])\n",
    "    plt.title('Top 10 Confusion Pairs')\n",
    "    plt.xlabel('True Tag -> Predicted Tag')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_pairs.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "342d681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hmm(tagger, test_data: List[List[Tuple[str, str]]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate HMM tagger using multiple metrics.\n",
    "    Args:\n",
    "        tagger: Trained HMM tagger\n",
    "        test_data: List of test sequences\n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_true_tags = []\n",
    "    all_predicted_tags = []\n",
    "    tag_counts = Counter()\n",
    "    reference_sequences = []\n",
    "    candidate_sequences = []\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for sentence in test_data:\n",
    "        words = [w for w, _ in sentence]\n",
    "        true_tags = [t for _, t in sentence]\n",
    "        predicted_tags = [t for _, t in tagger.tag(words)]\n",
    "        \n",
    "        all_true_tags.extend(true_tags)\n",
    "        all_predicted_tags.extend(predicted_tags)\n",
    "        \n",
    "        # Store sequences for BLEU and ROUGE\n",
    "        reference_sequences.append(true_tags)\n",
    "        candidate_sequences.append(predicted_tags)\n",
    "        \n",
    "        # Store sequence length\n",
    "        sequence_lengths.append(len(true_tags))\n",
    "        \n",
    "        # Update tag counts\n",
    "        tag_counts.update(true_tags)\n",
    "\n",
    "        for pred, true in zip(predicted_tags, true_tags):\n",
    "            total += 1\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Calculate sequence-level metrics\n",
    "    seq_metrics = evaluate_sequence_metrics(reference_sequences, candidate_sequences)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 for each tag\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true_tags, all_predicted_tags, average=None, labels=sorted(set(all_true_tags))\n",
    "    )\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_true_tags, all_predicted_tags, labels=sorted(set(all_true_tags)))\n",
    "    \n",
    "    # Calculate BLEU and ROUGE scores\n",
    "    bleu_score = evaluate_bleu(reference_sequences, candidate_sequences)\n",
    "    rouge_scores = evaluate_rouge(reference_sequences, candidate_sequences)\n",
    "    \n",
    "    # Analyze errors\n",
    "    error_analysis = analyze_errors(all_true_tags, all_predicted_tags)\n",
    "    \n",
    "    # Plot error analysis\n",
    "    plot_error_analysis(error_analysis['error_patterns'], error_analysis['confusion_pairs'])\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'sequence_metrics': seq_metrics,\n",
    "        'precision': dict(zip(sorted(set(all_true_tags)), precision)),\n",
    "        'recall': dict(zip(sorted(set(all_true_tags)), recall)),\n",
    "        'f1': dict(zip(sorted(set(all_true_tags)), f1)),\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': sorted(set(all_true_tags)),\n",
    "        'tag_counts': tag_counts,\n",
    "        'bleu': bleu_score,\n",
    "        'rouge': rouge_scores,\n",
    "        'error_analysis': error_analysis,\n",
    "        'sequence_lengths': sequence_lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fac357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, labels: List[str], title: str = 'Confusion Matrix'):\n",
    "    \"\"\"Plot confusion matrix with labels.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef8b73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_distribution(tag_counts: Counter):\n",
    "    \"\"\"Plot distribution of tags in the dataset.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tags, counts = zip(*tag_counts.most_common())\n",
    "    plt.bar(tags, counts)\n",
    "    plt.title('Tag Distribution in Dataset')\n",
    "    plt.xlabel('Tags')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tag_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7d21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_metrics(metrics: Dict):\n",
    "    \"\"\"Print evaluation metrics in a readable format.\"\"\"\n",
    "    print(f\"\\nOverall Accuracy: {metrics['accuracy']:.2%}\")\n",
    "    print(f\"\\nSequence-level Metrics:\")\n",
    "    print(f\"Exact Match: {metrics['sequence_metrics']['exact_match']:.2%}\")\n",
    "    print(f\"Partial Match: {metrics['sequence_metrics']['partial_match']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nBLEU Score: {metrics['bleu']:.4f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(f\"ROUGE-1: {metrics['rouge']['rouge-1']}\")\n",
    "    print(f\"ROUGE-2: {metrics['rouge']['rouge-2']}\")\n",
    "    print(f\"ROUGE-L: {metrics['rouge']['rouge-l']}\")\n",
    "    \n",
    "    print(\"\\nPer-tag metrics:\")\n",
    "    print(f\"{'Tag':<15} {'Count':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for tag in metrics['labels']:\n",
    "        count = metrics['tag_counts'][tag]\n",
    "        print(f\"{tag:<15} {count:<10} {metrics['precision'][tag]:<10.2%} {metrics['recall'][tag]:<10.2%} {metrics['f1'][tag]:<10.2%}\")\n",
    "    \n",
    "    print(\"\\nTop Error Patterns:\")\n",
    "    error_patterns = sorted(metrics['error_analysis']['error_patterns'].items(), \n",
    "                          key=lambda x: x[1], reverse=True)[:5]\n",
    "    for (true, pred), count in error_patterns:\n",
    "        print(f\"{true} -> {pred}: {count}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'], metrics['labels'])\n",
    "    \n",
    "    # Plot sequence length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(metrics['sequence_lengths'], bins=20)\n",
    "    plt.title('Sequence Length Distribution')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('sequence_lengths.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b24bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 73\n",
      "Training sequences: 58\n",
      "Test sequences: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n",
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 13.37%\n",
      "\n",
      "Sequence-level Metrics:\n",
      "Exact Match: 0.00%\n",
      "Partial Match: 13.33%\n",
      "\n",
      "BLEU Score: 0.0936\n",
      "\n",
      "ROUGE Scores:\n",
      "ROUGE-1: {'r': 0.3722222222222223, 'p': 0.9833333333333333, 'f': 0.49492063136780806}\n",
      "ROUGE-2: {'r': 0.18065564065564066, 'p': 0.7368686868686869, 'f': 0.22745217557014863}\n",
      "ROUGE-L: {'r': 0.35555555555555557, 'p': 0.95, 'f': 0.47269840914558586}\n",
      "\n",
      "Per-tag metrics:\n",
      "Tag             Count      Precision  Recall     F1        \n",
      "-------------------------------------------------------\n",
      "O               1385       27.27%     0.22%      0.43%     \n",
      "OBJ             168        50.00%     2.98%      5.62%     \n",
      "PRED            152        77.78%     4.61%      8.70%     \n",
      "SUBJ            254        12.80%     97.24%     22.63%    \n",
      "\n",
      "Top Error Patterns:\n",
      "O -> SUBJ: 1381\n",
      "OBJ -> SUBJ: 157\n",
      "PRED -> SUBJ: 144\n",
      "OBJ -> O: 6\n",
      "SUBJ -> OBJ: 5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = []\n",
    "    sectors = [\"finance\", \"tech\", \"healthcare\"]\n",
    "    for sector in sectors:\n",
    "        data.extend(load_and_preprocess_data(f'{sector}_data.csv'))\n",
    "    \n",
    "    if not data:\n",
    "        print(\"Error: No valid data found in the file.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    random.shuffle(data)\n",
    "    split_index = int(0.8 * len(data))\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    print(f\"Total sequences: {len(data)}\")\n",
    "    print(f\"Training sequences: {len(train_data)}\")\n",
    "    print(f\"Test sequences: {len(test_data)}\")\n",
    "    \n",
    "    # Train the HMM tagger\n",
    "    trainer = hmm.HiddenMarkovModelTrainer()\n",
    "    tagger = trainer.train_supervised(train_data)\n",
    "\n",
    "    # Evaluate the tagger\n",
    "    metrics = evaluate_hmm(tagger, test_data)\n",
    "    print_evaluation_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
