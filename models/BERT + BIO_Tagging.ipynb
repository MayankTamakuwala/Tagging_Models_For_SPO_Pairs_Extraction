{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm loading all the necessary libraries before proceeding to data loading and preprocessing.\n",
    "# ! python -m venv venv\n",
    "# ! source venv/bin/activate\n",
    "# %pip install pandas transformers torch scikit-learn hf_xet spacy rouge nltk requests seqeval\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "# OR\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import requests\n",
    "import os\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights already exists\n"
     ]
    }
   ],
   "source": [
    "# I'm making sure the pretrained model weights are downloaded if not already present.\n",
    "if not os.path.exists(\"./BERT_BIO_Tagging_model.pth\"):\n",
    "    # URL of my HuggingFace account where I've uploaded the trained model weights.\n",
    "    url = \"https://huggingface.co/MayankTamakuwala/BERT_BIO_Tagger/resolve/main/BERT_BIO_Tagging_model.pth\"\n",
    "    output_path = \"BERT_BIO_Tagging_model.pth\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    print(f\"Model weights downloaded to {output_path}\")\n",
    "else:\n",
    "    print(f\"Weights already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am combining all sector-specific articles into a single DataFrame for processing.\n",
    "articles_df = pd.DataFrame()\n",
    "sectors = [\"finance\", \"healthcare\", \"tech\"]\n",
    "triplets_list = []\n",
    "for sector in sectors:\n",
    "    articles = pd.read_csv(f\"../Webscraped Dataset/globenewswire_articles_{sector}.csv\")\n",
    "    articles_df = pd.concat([articles_df, articles], ignore_index=True)\n",
    "\n",
    "    with open(f\"../Ground Truth/{sector}_articles_triplets.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            \n",
    "            url = row[0]\n",
    "            triplet_fields = []\n",
    "            \n",
    "            for field in row[1:]:\n",
    "                if field.strip():\n",
    "                    str_tuple = field.strip()\n",
    "                    if str_tuple.startswith('(') and str_tuple.endswith(')'):\n",
    "                        inner_str = str_tuple[1:-1]\n",
    "                        elements = [elem.strip() for elem in inner_str.split(',')]\n",
    "                        triplet_fields.append(tuple(elements))\n",
    "                    else:\n",
    "                        triplet_fields.append((str_tuple,))\n",
    "\n",
    "            triplets_list.append({\"url\": url, \"triplets\": triplet_fields})\n",
    "\n",
    "triplets_df = pd.DataFrame(triplets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tags(text, spo_list):\n",
    "    # I'm trying to align SPO triplets with token spans to generate BIO tags.\n",
    "    tokens = tokenize_text(text)\n",
    "    tags = ['O'] * len(tokens)\n",
    "\n",
    "    for spo in spo_list:\n",
    "        try:\n",
    "            subject, predicate, obj = spo\n",
    "            spans = {\n",
    "                'SUB': subject.split(),\n",
    "                'PRED': predicate.split(),\n",
    "                'OBJ': obj.split()\n",
    "            }\n",
    "\n",
    "            for label, span_tokens in spans.items():\n",
    "                for i in range(len(tokens) - len(span_tokens) + 1):\n",
    "                    if tokens[i:i+len(span_tokens)] == span_tokens:\n",
    "                        tags[i] = f'B-{label}'\n",
    "                        for j in range(1, len(span_tokens)):\n",
    "                            tags[i + j] = f'I-{label}'\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm now creating the final dataset by aligning tokens and their corresponding BIO tags.\n",
    "dataset = []\n",
    "\n",
    "for idx, row in articles_df.iterrows():\n",
    "    url = row['url']\n",
    "    content = row['content']\n",
    "\n",
    "    matching_triplets_row = triplets_df[triplets_df['url'] == url]\n",
    "    if matching_triplets_row.empty:\n",
    "        continue\n",
    "\n",
    "    triplets = triplets_df.iloc[idx, 1]\n",
    "    tokens, tags = get_bio_tags(content, triplets)\n",
    "    dataset.append((tokens, triplets, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I'm initializing the tokenizer and defining tag mappings for the BIO tagging scheme.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# # Unique BIO tags\n",
    "tag_values = ['O', 'B-SUB', 'I-SUB', 'B-PRED', 'I-PRED', 'B-OBJ', 'I-OBJ']\n",
    "tag2id = {tag: i for i, tag in enumerate(tag_values)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print (\"MPS device found.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print (\"CUDA device found.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print (\"Using CPU.\")\n",
    "\n",
    "class BERT_SPO_BIO_Tagger(nn.Module):\n",
    "    # I am defining my custom BERT-based model for sequence tagging using BIO labels.\n",
    "    def __init__(self, tag2id, id2tag, tokenizer, lr=5e-5, epochs = 10):\n",
    "        super(BERT_SPO_BIO_Tagger, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, len(tag2id))\n",
    "        self.__tag2id = tag2id\n",
    "        self.__train_loader = None\n",
    "        self.__id2tag = id2tag\n",
    "        self.__val_loader = None\n",
    "        self.__lr = lr\n",
    "        self.__epochs = epochs\n",
    "        self.__tokenizer = tokenizer\n",
    "        self.__nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        class SPOBioDataset(Dataset):\n",
    "            def __init__(self, data, tokenizer, tag2id, max_len=512):\n",
    "                self.data = data\n",
    "                self.tokenizer = tokenizer\n",
    "                self.tag2id = tag2id\n",
    "                self.max_len = max_len\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.data)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                tokens, _, tags  = self.data[idx]\n",
    "\n",
    "                tokenized_input = self.tokenizer(tokens,\n",
    "                                                is_split_into_words=True,\n",
    "                                                padding='max_length',\n",
    "                                                truncation=True,\n",
    "                                                max_length=self.max_len,\n",
    "                                                return_tensors=\"pt\")\n",
    "\n",
    "                word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "                label_ids = []\n",
    "\n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    else:\n",
    "                        label_ids.append(self.tag2id.get(tags[word_idx], self.tag2id['O']))\n",
    "\n",
    "                return {\n",
    "                    'input_ids': tokenized_input['input_ids'].squeeze(),\n",
    "                    'attention_mask': tokenized_input['attention_mask'].squeeze(),\n",
    "                    'labels': torch.tensor(label_ids)\n",
    "                }\n",
    "\n",
    "            def get_raw_item(self, idx):\n",
    "                return self.data[idx] \n",
    "            \n",
    "        train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "        train_dataset = SPOBioDataset(train_data, self.__tokenizer, self.__tag2id)\n",
    "        val_dataset = SPOBioDataset(val_data, self.__tokenizer, self.__tag2id)\n",
    "\n",
    "        self.__train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        self.__val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "        \n",
    "        weights = self.__compute_class_weights(train_data)\n",
    "\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.__lr)\n",
    "        loss_func = nn.CrossEntropyLoss(ignore_index=-100, weight=weights)\n",
    "\n",
    "        # Finally, I'm starting the training loop to fine-tune the model on my custom dataset.\n",
    "        for epoch in range(self.__epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in self.__train_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = self(input_ids, attention_mask)\n",
    "\n",
    "                loss = loss_func(logits.view(-1, len(self.__tag2id)), labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(self.__train_loader)\n",
    "            print(f\"Epoch {epoch + 1} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __compute_class_weights(self, dataset):\n",
    "        tag_counts = Counter(tag for _, _, tags in dataset for tag in tags)\n",
    "        total = sum(tag_counts.values())\n",
    "        weights = [1.0 - (tag_counts[tag] / total) for tag in self.__tag2id.keys()]\n",
    "        # weights[0] += 0.14\n",
    "        weights[0] += 0.1\n",
    "        return torch.tensor(weights).to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_on_validation_data(self):\n",
    "        # I am now evaluating the model using F1, Precision, Recall, ROUGE, and BLEU metrics.\n",
    "        self.eval()\n",
    "        seqeval_true = []\n",
    "        seqeval_pred = []\n",
    "\n",
    "        val_dataset = self.__val_loader.dataset\n",
    "\n",
    "        for batch in self.__val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                true_tags = []\n",
    "                pred_tags = []\n",
    "                for j in range(len(labels[i])):\n",
    "                    if labels[i][j] != -100:\n",
    "                        true_tag = self.__id2tag[labels[i][j].item()]\n",
    "                        pred_tag = self.__id2tag[predictions[i][j].item()]\n",
    "                        true_tags.append(true_tag)\n",
    "                        pred_tags.append(pred_tag)\n",
    "                seqeval_true.append(true_tags)\n",
    "                seqeval_pred.append(pred_tags)\n",
    "\n",
    "        seqeval_metrics = {\n",
    "            \"classification_report\": classification_report(\n",
    "                [tag for seq in seqeval_true for tag in seq],\n",
    "                [tag for seq in seqeval_pred for tag in seq],\n",
    "                digits=4),\n",
    "            \"f1\": f1_score(seqeval_true, seqeval_pred),\n",
    "            \"precision\": precision_score(seqeval_true, seqeval_pred),\n",
    "            \"recall\": recall_score(seqeval_true, seqeval_pred)\n",
    "        }\n",
    "\n",
    "        predicted_triplets_all = []\n",
    "        reference_triplets_all = []\n",
    "\n",
    "        val_dataset = self.__val_loader.dataset\n",
    "\n",
    "        for i in range(len(val_dataset)):\n",
    "            tokens, true_triplets, _ = val_dataset.get_raw_item(i)\n",
    "            text = \" \".join(tokens)\n",
    "            pred_tags = self.__predict_bio_tags(tokens)\n",
    "            pred_triplets = self.__extract_and_form_triplets(text, pred_tags)\n",
    "\n",
    "            predicted_triplets_all.append(\" \".join([\" \".join(triplet) for triplet in pred_triplets]))\n",
    "            reference_triplets_all.append(\" \".join([\" \".join(triplet) for triplet in true_triplets]))\n",
    "\n",
    "        rouge = Rouge()\n",
    "        rouge_scores = []\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        bleu_scores = []\n",
    "\n",
    "        for ref, pred in zip(reference_triplets_all, predicted_triplets_all):\n",
    "            try:\n",
    "                score = rouge.get_scores(pred, ref)[0]\n",
    "                rouge_scores.append(score)\n",
    "            except:\n",
    "                continue\n",
    "            bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
    "            bleu_scores.append(bleu)\n",
    "\n",
    "        avg_rouge = {}\n",
    "        if rouge_scores:\n",
    "            keys = rouge_scores[0].keys()\n",
    "            for k in keys:\n",
    "                avg_rouge[k] = {\n",
    "                    \"f\": sum(d[k][\"f\"] for d in rouge_scores) / len(rouge_scores),\n",
    "                    \"p\": sum(d[k][\"p\"] for d in rouge_scores) / len(rouge_scores),\n",
    "                    \"r\": sum(d[k][\"r\"] for d in rouge_scores) / len(rouge_scores),\n",
    "                }\n",
    "\n",
    "        avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "        return {\n",
    "            \"seqeval\": seqeval_metrics,\n",
    "            \"ROUGE\": avg_rouge,\n",
    "            \"BLEU\": avg_bleu\n",
    "        }\n",
    "\n",
    "    def load_model_weights(self, torch_load_weights):\n",
    "        self.load_state_dict(torch_load_weights)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __predict_bio_tags(self, tokens):\n",
    "        self.eval()\n",
    "\n",
    "        tokenized_input = self.__tokenizer(tokens,\n",
    "                                    is_split_into_words=True,\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    truncation=True,\n",
    "                                    padding=\"max_length\",\n",
    "                                    max_length=512)\n",
    "\n",
    "        input_ids = tokenized_input[\"input_ids\"].to(device)\n",
    "        attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
    "\n",
    "        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "        predicted_tags = []\n",
    "\n",
    "        for idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            tag_id = predictions[0][idx].item()\n",
    "            tag = self.__id2tag[tag_id]\n",
    "            if idx == 0 or word_ids[idx] != word_ids[idx - 1]:\n",
    "                predicted_tags.append((tokens[word_idx], tag))\n",
    "\n",
    "        return predicted_tags\n",
    "\n",
    "    def __extract_and_form_triplets(self, text, tagged_tokens):\n",
    "        # I'm reconstructing SPO triplets by checking which entities co-occur in the same sentence.\n",
    "        spans = {'SUB': [], 'PRED': [], 'OBJ': []}\n",
    "        current_span = []\n",
    "        current_label = None\n",
    "\n",
    "        for token, tag in tagged_tokens:\n",
    "            if tag == 'O':\n",
    "                if current_span and current_label:\n",
    "                    spans[current_label].append(\" \".join(current_span))\n",
    "                current_span = []\n",
    "                current_label = None\n",
    "            elif tag.startswith('B-'):\n",
    "                if current_span and current_label:\n",
    "                    spans[current_label].append(\" \".join(current_span))\n",
    "                current_label = tag[2:]\n",
    "                current_span = [token]\n",
    "            elif tag.startswith('I-') and current_label == tag[2:]:\n",
    "                current_span.append(token)\n",
    "            else:\n",
    "                if current_span and current_label:\n",
    "                    spans[current_label].append(\" \".join(current_span))\n",
    "                current_span = []\n",
    "                current_label = None\n",
    "\n",
    "        if current_span and current_label:\n",
    "            spans[current_label].append(\" \".join(current_span))\n",
    "\n",
    "        filtered_spans = spans\n",
    "\n",
    "        doc = self.__nlp(text)\n",
    "        triplets = []\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text\n",
    "            subjs = [s for s in filtered_spans[\"SUB\"] if s in sent_text]\n",
    "            preds = [p for p in filtered_spans[\"PRED\"] if p in sent_text]\n",
    "            objs = [o for o in filtered_spans[\"OBJ\"] if o in sent_text]\n",
    "\n",
    "            for s in subjs:\n",
    "                for p in preds:\n",
    "                    for o in objs:\n",
    "                        triplets.append((s, p, o))\n",
    "\n",
    "        return list(set(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_SPO_BIO_Tagger(tag2id, id2tag, tokenizer, epochs = 0).to(device)\n",
    "\n",
    "# to actually train the model, increase the nuber of epochs \n",
    "# comment the line below to not load the pretrained weights\n",
    "model.load_model_weights(torch.load(\"BERT_BIO_Tagging_model.pth\"))\n",
    "\n",
    "model = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_on_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-OBJ     0.2447    0.1060    0.1479       217\n",
      "      B-PRED     0.2083    0.4651    0.2878        43\n",
      "       B-SUB     0.4956    0.5773    0.5333        97\n",
      "       I-OBJ     0.1818    0.0654    0.0962       153\n",
      "      I-PRED     0.1429    0.0769    0.1000        13\n",
      "       I-SUB     0.4080    0.6375    0.4976        80\n",
      "           O     0.9649    0.9748    0.9698     10964\n",
      "\n",
      "    accuracy                         0.9379     11567\n",
      "   macro avg     0.3780    0.4147    0.3761     11567\n",
      "weighted avg     0.9295    0.9379    0.9324     11567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(scores[\"seqeval\"][\"classification_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.24615384615384617)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"seqeval\"][\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.24581005586592178)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"seqeval\"][\"precision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.24649859943977592)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"seqeval\"][\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.31441401150228077,\n",
       "  'p': 0.5647741147741148,\n",
       "  'r': 0.23907931293059087},\n",
       " 'rouge-2': {'f': 0.12060935075891474,\n",
       "  'p': 0.23598852128263892,\n",
       "  'r': 0.09874577657313044},\n",
       " 'rouge-l': {'f': 0.2863798887622757,\n",
       "  'p': 0.5159879336349924,\n",
       "  'r': 0.21681300587726565}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"ROUGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036403264031039814"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"BLEU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_bio_tags(text, model, tokenizer, id2tag, device):\n",
    "    model.eval()\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    tokenized_input = tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=512)\n",
    "\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    predicted_tags = []\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        tag_id = predictions[0][idx].item()\n",
    "        tag = id2tag[tag_id]\n",
    "        if idx == 0 or word_ids[idx] != word_ids[idx - 1]:\n",
    "            predicted_tags.append((tokens[word_idx], tag))\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "def extract_and_form_triplets(text, tagged_tokens):\n",
    "\n",
    "    # Step 1: Extract spans from BIO-tagged tokens\n",
    "    spans = {'SUB': [], 'PRED': [], 'OBJ': []}\n",
    "    current_span = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag == 'O':\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_span = []\n",
    "            current_label = None\n",
    "        elif tag.startswith('B-'):\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_label = tag[2:]\n",
    "            current_span = [token]\n",
    "        elif tag.startswith('I-') and current_label == tag[2:]:\n",
    "            current_span.append(token)\n",
    "        else:\n",
    "            if current_span and current_label:\n",
    "                spans[current_label].append(\" \".join(current_span))\n",
    "            current_span = []\n",
    "            current_label = None\n",
    "\n",
    "    if current_span and current_label:\n",
    "        spans[current_label].append(\" \".join(current_span))\n",
    "\n",
    "    # Step 2: Filter out short or lowercase-only spans\n",
    "    # def filter_spans(spans):\n",
    "    #     def is_valid(span):\n",
    "    #         return len(span.split()) > 1 or span[0].isupper()\n",
    "\n",
    "    #     return {\n",
    "    #         k: [s for s in v if is_valid(s)] for k, v in spans.items()\n",
    "    #     }\n",
    "\n",
    "    # filtered_spans = filter_spans(spans)\n",
    "\n",
    "    filtered_spans = spans\n",
    "\n",
    "    # Step 3: Match spans within same sentence\n",
    "    doc = nlp(text)\n",
    "    triplets = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text\n",
    "        subjs = [s for s in filtered_spans[\"SUB\"] if s in sent_text]\n",
    "        preds = [p for p in filtered_spans[\"PRED\"] if p in sent_text]\n",
    "        objs = [o for o in filtered_spans[\"OBJ\"] if o in sent_text]\n",
    "\n",
    "        for s in subjs:\n",
    "            for p in preds:\n",
    "                for o in objs:\n",
    "                    triplets.append((s, p, o))\n",
    "\n",
    "    return list(set(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Triplets:\n",
      "('ibex', 'said', 'Chief Strategic Accounts Officer and CMO at ibex')\n",
      "('ibex', 'is', 'online acquisition')\n",
      "('Julie Casteel', 'said', 'CCW Executive Exchange')\n",
      "('ibex', 'is', 'global CX delivery center')\n",
      "('ibex', 'operates', '30 operations')\n",
      "('ibex', 'delivers', 'digital marketing')\n",
      "('ibex', 'said', 'CCW Executive Exchange')\n",
      "('ibex', 'is', 'superior')\n",
      "('ibex', 'acquire', 'digital marketing')\n",
      "('ibex', 'operates', 'global CX delivery center')\n",
      "('ibex', 'help', 'digital marketing')\n",
      "('ibex', 'delivers', 'online acquisition')\n",
      "('ibex', 'is', '30 operations')\n",
      "('Julie Casteel', 'said', 'Chief Strategic Accounts Officer and CMO at ibex')\n",
      "('ibex', 'acquire', 'online acquisition')\n",
      "('ibex', 'operates', 'superior')\n",
      "('ibex', 'is', 'digital marketing')\n",
      "('ibex', 'help', 'online acquisition')\n"
     ]
    }
   ],
   "source": [
    "# Finally, I'm running inference on a random sample to see the predicted SPO triplets.\n",
    "sample_text = articles_df.iloc[random.randint(0, articles_df.shape[0] - 1)][\"content\"]\n",
    "\n",
    "tagged = predict_bio_tags(sample_text, model, tokenizer, id2tag, device)\n",
    "triplets = extract_and_form_triplets(sample_text, tagged)\n",
    "\n",
    "print(\"Predicted Triplets:\")\n",
    "for t in triplets:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
