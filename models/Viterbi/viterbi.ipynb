{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8fff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m venv venv\n",
    "# ! source venv/bin/activate\n",
    "# %pip install pandas scikit-learn rouge nltk numpy matplotlib seaborn\n",
    "# OR\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0e44d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mayanktamakuwala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from nltk.tag import hmm\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1990887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path: str) -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Load and preprocess the data from CSV file.\n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "    Returns:\n",
    "        List of sequences, where each sequence is a list of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        \n",
    "        for row in reader:\n",
    "            if not row:  # Skip empty rows\n",
    "                continue\n",
    "                \n",
    "            # Get non-empty columns\n",
    "            selected_columns = [col for col in row[1:] if col.strip()]\n",
    "            \n",
    "            # Process each column to create word-tag pairs\n",
    "            curr = []\n",
    "            for s in selected_columns:\n",
    "                try:\n",
    "                    parts = s.split(\",\")\n",
    "                    if len(parts) != 2:\n",
    "                        continue\n",
    "                        \n",
    "                    word = parts[0].strip().lower()\n",
    "                    tag = parts[1].strip()\n",
    "                    \n",
    "                    if word and tag:  # Only add if both word and tag are non-empty\n",
    "                        curr.append((word, tag))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing column '{s}': {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if curr:  # Only add sequences that have at least one valid word-tag pair\n",
    "                data.append(curr)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a72445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(reference: List[List[str]], candidate: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between reference and candidate sequences.\n",
    "    Args:\n",
    "        reference: List of reference sequences\n",
    "        candidate: List of candidate sequences\n",
    "    Returns:\n",
    "        BLEU score\n",
    "    \"\"\"\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    \n",
    "    for ref, cand in zip(reference, candidate):\n",
    "        # Convert sequences to strings and tokenize\n",
    "        ref_str = ' '.join(ref)\n",
    "        cand_str = ' '.join(cand)\n",
    "        \n",
    "        ref_tokens = nltk.word_tokenize(ref_str)\n",
    "        cand_tokens = nltk.word_tokenize(cand_str)\n",
    "        \n",
    "        # Calculate BLEU score for this pair\n",
    "        score = sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Return average BLEU score\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485f66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(reference: List[List[str]], candidate: List[List[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores between reference and candidate sequences.\n",
    "    Args:\n",
    "        reference: List of reference sequences\n",
    "        candidate: List of candidate sequences\n",
    "    Returns:\n",
    "        Dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    # Convert sequences to sentences\n",
    "    reference_sentences = [' '.join(seq) for seq in reference]\n",
    "    candidate_sentences = [' '.join(seq) for seq in candidate]\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate_sentences, reference_sentences, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6481b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sequence_metrics(reference: List[List[str]], candidate: List[List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate sequence-level metrics.\n",
    "    Args:\n",
    "        reference: List of reference sequences\n",
    "        candidate: List of candidate sequences\n",
    "    Returns:\n",
    "        Dictionary containing sequence-level metrics\n",
    "    \"\"\"\n",
    "    exact_matches = 0\n",
    "    partial_matches = 0\n",
    "    total_sequences = len(reference)\n",
    "    \n",
    "    for ref, cand in zip(reference, candidate):\n",
    "        # Exact match\n",
    "        if ref == cand:\n",
    "            exact_matches += 1\n",
    "        \n",
    "        # Partial match (at least 50% of tags correct)\n",
    "        correct_tags = sum(1 for r, c in zip(ref, cand) if r == c)\n",
    "        if correct_tags / len(ref) >= 0.5:\n",
    "            partial_matches += 1\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_matches / total_sequences if total_sequences > 0 else 0,\n",
    "        'partial_match': partial_matches / total_sequences if total_sequences > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042dea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(true_tags: List[str], pred_tags: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze error patterns in predictions.\n",
    "    Args:\n",
    "        true_tags: List of true tags\n",
    "        pred_tags: List of predicted tags\n",
    "    Returns:\n",
    "        Dictionary containing error analysis\n",
    "    \"\"\"\n",
    "    error_patterns = defaultdict(int)\n",
    "    confusion_pairs = defaultdict(int)\n",
    "    \n",
    "    for true, pred in zip(true_tags, pred_tags):\n",
    "        if true != pred:\n",
    "            error_patterns[(true, pred)] += 1\n",
    "            confusion_pairs[f\"{true}->{pred}\"] += 1\n",
    "    \n",
    "    return {\n",
    "        'error_patterns': dict(error_patterns),\n",
    "        'confusion_pairs': dict(confusion_pairs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943cf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_analysis(error_patterns: Dict, confusion_pairs: Dict):\n",
    "    \"\"\"Plot error analysis visualizations.\"\"\"\n",
    "    # Plot most common error patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    patterns = sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    plt.bar([str(p) for p, _ in patterns], [c for _, c in patterns])\n",
    "    plt.title('Top 10 Error Patterns')\n",
    "    plt.xlabel('(True Tag, Predicted Tag)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_patterns.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion pairs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    plt.bar([p for p, _ in pairs], [c for _, c in pairs])\n",
    "    plt.title('Top 10 Confusion Pairs')\n",
    "    plt.xlabel('True Tag -> Predicted Tag')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_pairs.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342d681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hmm(tagger, test_data: List[List[Tuple[str, str]]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate HMM tagger using multiple metrics.\n",
    "    Args:\n",
    "        tagger: Trained HMM tagger\n",
    "        test_data: List of test sequences\n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_true_tags = []\n",
    "    all_predicted_tags = []\n",
    "    tag_counts = Counter()\n",
    "    reference_sequences = []\n",
    "    candidate_sequences = []\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for sentence in test_data:\n",
    "        words = [w for w, _ in sentence]\n",
    "        true_tags = [t for _, t in sentence]\n",
    "        predicted_tags = [t for _, t in tagger.tag(words)]\n",
    "        \n",
    "        all_true_tags.extend(true_tags)\n",
    "        all_predicted_tags.extend(predicted_tags)\n",
    "        \n",
    "        # Store sequences for BLEU and ROUGE\n",
    "        reference_sequences.append(true_tags)\n",
    "        candidate_sequences.append(predicted_tags)\n",
    "        \n",
    "        # Store sequence length\n",
    "        sequence_lengths.append(len(true_tags))\n",
    "        \n",
    "        # Update tag counts\n",
    "        tag_counts.update(true_tags)\n",
    "\n",
    "        for pred, true in zip(predicted_tags, true_tags):\n",
    "            total += 1\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Calculate sequence-level metrics\n",
    "    seq_metrics = evaluate_sequence_metrics(reference_sequences, candidate_sequences)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 for each tag\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true_tags, all_predicted_tags, average=None, labels=sorted(set(all_true_tags))\n",
    "    )\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_true_tags, all_predicted_tags, labels=sorted(set(all_true_tags)))\n",
    "    \n",
    "    # Calculate BLEU and ROUGE scores\n",
    "    bleu_score = evaluate_bleu(reference_sequences, candidate_sequences)\n",
    "    rouge_scores = evaluate_rouge(reference_sequences, candidate_sequences)\n",
    "    \n",
    "    # Analyze errors\n",
    "    error_analysis = analyze_errors(all_true_tags, all_predicted_tags)\n",
    "    \n",
    "    # Plot error analysis\n",
    "    plot_error_analysis(error_analysis['error_patterns'], error_analysis['confusion_pairs'])\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'sequence_metrics': seq_metrics,\n",
    "        'precision': dict(zip(sorted(set(all_true_tags)), precision)),\n",
    "        'recall': dict(zip(sorted(set(all_true_tags)), recall)),\n",
    "        'f1': dict(zip(sorted(set(all_true_tags)), f1)),\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': sorted(set(all_true_tags)),\n",
    "        'tag_counts': tag_counts,\n",
    "        'bleu': bleu_score,\n",
    "        'rouge': rouge_scores,\n",
    "        'error_analysis': error_analysis,\n",
    "        'sequence_lengths': sequence_lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fac357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, labels: List[str], title: str = 'Confusion Matrix'):\n",
    "    \"\"\"Plot confusion matrix with labels.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8b73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_distribution(tag_counts: Counter):\n",
    "    \"\"\"Plot distribution of tags in the dataset.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tags, counts = zip(*tag_counts.most_common())\n",
    "    plt.bar(tags, counts)\n",
    "    plt.title('Tag Distribution in Dataset')\n",
    "    plt.xlabel('Tags')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tag_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7d21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_metrics(metrics: Dict):\n",
    "    \"\"\"Print evaluation metrics in a readable format.\"\"\"\n",
    "    print(f\"\\nOverall Accuracy: {metrics['accuracy']:.2%}\")\n",
    "    print(f\"\\nSequence-level Metrics:\")\n",
    "    print(f\"Exact Match: {metrics['sequence_metrics']['exact_match']:.2%}\")\n",
    "    print(f\"Partial Match: {metrics['sequence_metrics']['partial_match']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nBLEU Score: {metrics['bleu']:.4f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(f\"ROUGE-1: {metrics['rouge']['rouge-1']}\")\n",
    "    print(f\"ROUGE-2: {metrics['rouge']['rouge-2']}\")\n",
    "    print(f\"ROUGE-L: {metrics['rouge']['rouge-l']}\")\n",
    "    \n",
    "    print(\"\\nPer-tag metrics:\")\n",
    "    print(f\"{'Tag':<15} {'Count':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for tag in metrics['labels']:\n",
    "        count = metrics['tag_counts'][tag]\n",
    "        print(f\"{tag:<15} {count:<10} {metrics['precision'][tag]:<10.2%} {metrics['recall'][tag]:<10.2%} {metrics['f1'][tag]:<10.2%}\")\n",
    "    \n",
    "    print(\"\\nTop Error Patterns:\")\n",
    "    error_patterns = sorted(metrics['error_analysis']['error_patterns'].items(), \n",
    "                          key=lambda x: x[1], reverse=True)[:5]\n",
    "    for (true, pred), count in error_patterns:\n",
    "        print(f\"{true} -> {pred}: {count}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'], metrics['labels'])\n",
    "    \n",
    "    # Plot sequence length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(metrics['sequence_lengths'], bins=20)\n",
    "    plt.title('Sequence Length Distribution')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('sequence_lengths.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b24bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n",
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 246\n",
      "Training sequences: 196\n",
      "Test sequences: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayanktamakuwala/Documents/PHI/Git/Knowledge-Graphs/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 18.89%\n",
      "\n",
      "Sequence-level Metrics:\n",
      "Exact Match: 0.00%\n",
      "Partial Match: 10.00%\n",
      "\n",
      "BLEU Score: 0.1137\n",
      "\n",
      "ROUGE Scores:\n",
      "ROUGE-1: {'r': 0.43652380952380954, 'p': 0.99, 'f': 0.5508571391478871}\n",
      "ROUGE-2: {'r': 0.22729136060018423, 'p': 0.7782222222222223, 'f': 0.27840664995730136}\n",
      "ROUGE-L: {'r': 0.40319047619047615, 'p': 0.9566666666666667, 'f': 0.5179999962907443}\n",
      "\n",
      "Per-tag metrics:\n",
      "Tag             Count      Precision  Recall     F1        \n",
      "-------------------------------------------------------\n",
      "O               2922       74.16%     2.26%      4.38%     \n",
      "OBJ             805        44.44%     0.99%      1.94%     \n",
      "ORG             36         0.00%      0.00%      0.00%     \n",
      "PERSON          1          0.00%      0.00%      0.00%     \n",
      "PLACE           4          0.00%      0.00%      0.00%     \n",
      "PRED            740        89.66%     3.51%      6.76%     \n",
      "SUBJ            940        17.49%     98.83%     29.72%    \n",
      "\n",
      "Top Error Patterns:\n",
      "O -> SUBJ: 2851\n",
      "OBJ -> SUBJ: 781\n",
      "PRED -> SUBJ: 710\n",
      "ORG -> SUBJ: 36\n",
      "OBJ -> O: 16\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = []\n",
    "    sectors = [\"finance\", \"tech\", \"healthcare\"]\n",
    "    for sector in sectors:\n",
    "        data.extend(load_and_preprocess_data(f'./viterbi_processed_data/{sector}_data.csv'))\n",
    "    \n",
    "    if not data:\n",
    "        print(\"Error: No valid data found in the file.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    random.shuffle(data)\n",
    "    split_index = int(0.8 * len(data))\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    print(f\"Total sequences: {len(data)}\")\n",
    "    print(f\"Training sequences: {len(train_data)}\")\n",
    "    print(f\"Test sequences: {len(test_data)}\")\n",
    "    \n",
    "    # Train the HMM tagger\n",
    "    trainer = hmm.HiddenMarkovModelTrainer()\n",
    "    tagger = trainer.train_supervised(train_data)\n",
    "\n",
    "    # Evaluate the tagger\n",
    "    metrics = evaluate_hmm(tagger, test_data)\n",
    "    print_evaluation_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
